{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy import stats\n",
    "import utils\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "\n",
    "import model.net as net\n",
    "from dataloader import *\n",
    "from train import train_and_evaluate\n",
    "\n",
    "months = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 918/918 [00:27<00:00, 33.64it/s]\n"
     ]
    }
   ],
   "source": [
    "data_set = 'Irish_2010'\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "\n",
    "data = get_data(path, data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = 1\n",
    "n_clusters = 2\n",
    "method = 'hierarchical/euclidean'\n",
    "\n",
    "path_cluster = os.path.join(path, 'result', data_set, 'clustering', 'point', method, f'n_clusters_{n_clusters}.csv')\n",
    "clusters = pd.read_csv(path_cluster, header=None)\n",
    "path_data = os.path.join(path, 'data', 'deepar')\n",
    "\n",
    "series = data[:, month-1, :months[month-1]*24]\n",
    "\n",
    "weather = get_weather(path, data_set, month)\n",
    "week = get_dow(data_set, month)\n",
    "day = get_hod(month)\n",
    "\n",
    "num_covariates = 4\n",
    "covariates = np.zeros((num_covariates, len(series[0])))\n",
    "covariates[1] = stats.zscore(weather)\n",
    "covariates[2] = stats.zscore(week)\n",
    "covariates[3] = stats.zscore(day)\n",
    "covariates = covariates.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training\n",
      "Epoch 1/100\n",
      "train_loss: 0.6607468223347911\n",
      "Validation loss decreased (inf --> 0.443681).  Saving model ...\n",
      "Epoch 2/100\n",
      "train_loss: 0.32175363754162767\n",
      "Validation loss decreased (0.443681 --> 0.225415).  Saving model ...\n",
      "Epoch 3/100\n",
      "train_loss: 0.16878689153933188\n",
      "Validation loss decreased (0.225415 --> 0.137206).  Saving model ...\n",
      "Epoch 4/100\n",
      "train_loss: 0.10432238997498029\n",
      "Validation loss decreased (0.137206 --> 0.081638).  Saving model ...\n",
      "Epoch 5/100\n",
      "train_loss: 0.06488419300876558\n",
      "Validation loss decreased (0.081638 --> 0.046449).  Saving model ...\n",
      "Epoch 6/100\n",
      "train_loss: 0.04017331458610688\n",
      "Validation loss decreased (0.046449 --> 0.017404).  Saving model ...\n",
      "Epoch 7/100\n",
      "train_loss: 0.02089914055858157\n",
      "Validation loss decreased (0.017404 --> 0.005875).  Saving model ...\n",
      "Epoch 8/100\n",
      "train_loss: -0.009772424693241223\n",
      "Validation loss decreased (0.005875 --> -0.005540).  Saving model ...\n",
      "Epoch 9/100\n",
      "train_loss: -0.013716827171850302\n",
      "Validation loss decreased (-0.005540 --> -0.015436).  Saving model ...\n",
      "Epoch 10/100\n",
      "train_loss: -0.027128772901922043\n",
      "Validation loss decreased (-0.015436 --> -0.044645).  Saving model ...\n",
      "Epoch 11/100\n",
      "train_loss: -0.037981863461539776\n",
      "Validation loss decreased (-0.044645 --> -0.045757).  Saving model ...\n",
      "Epoch 12/100\n",
      "train_loss: -0.054890632383446665\n",
      "Validation loss decreased (-0.045757 --> -0.062314).  Saving model ...\n",
      "Epoch 13/100\n",
      "train_loss: -0.06976032621957397\n",
      "Validation loss decreased (-0.062314 --> -0.070258).  Saving model ...\n",
      "Epoch 14/100\n",
      "train_loss: -0.07215935798080969\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 15/100\n",
      "train_loss: -0.08277350821545428\n",
      "Validation loss decreased (-0.070258 --> -0.091809).  Saving model ...\n",
      "Epoch 16/100\n",
      "train_loss: -0.08573215005770675\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 17/100\n",
      "train_loss: -0.09837049423354054\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 18/100\n",
      "train_loss: -0.09959222785822452\n",
      "Validation loss decreased (-0.091809 --> -0.114345).  Saving model ...\n",
      "Epoch 19/100\n",
      "train_loss: -0.11821323757370313\n",
      "Validation loss decreased (-0.114345 --> -0.124176).  Saving model ...\n",
      "Epoch 20/100\n",
      "train_loss: -0.11601455557759098\n",
      "Validation loss decreased (-0.124176 --> -0.129384).  Saving model ...\n",
      "Epoch 21/100\n",
      "train_loss: -0.1160367218331552\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 22/100\n",
      "train_loss: -0.1354052574623009\n",
      "Validation loss decreased (-0.129384 --> -0.138248).  Saving model ...\n",
      "Epoch 23/100\n",
      "train_loss: -0.14195519384923674\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 24/100\n",
      "train_loss: -0.13241652360824352\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 25/100\n",
      "train_loss: -0.14653224484041824\n",
      "Validation loss decreased (-0.138248 --> -0.144866).  Saving model ...\n",
      "Epoch 26/100\n",
      "train_loss: -0.14726675056935476\n",
      "Validation loss decreased (-0.144866 --> -0.151795).  Saving model ...\n",
      "Epoch 27/100\n",
      "train_loss: -0.15387910430455154\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 28/100\n",
      "train_loss: -0.1588946953066078\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 29/100\n",
      "train_loss: -0.1679329798500023\n",
      "Validation loss decreased (-0.151795 --> -0.168356).  Saving model ...\n",
      "Epoch 30/100\n",
      "train_loss: -0.16706539459631475\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 31/100\n",
      "train_loss: -0.16473692165853832\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 32/100\n",
      "train_loss: -0.1769661189454822\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 33/100\n",
      "train_loss: -0.17805897937694065\n",
      "Validation loss decreased (-0.168356 --> -0.188383).  Saving model ...\n",
      "Epoch 34/100\n",
      "train_loss: -0.1867981419806749\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 35/100\n",
      "train_loss: -0.19324232204139513\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 36/100\n",
      "train_loss: -0.19107764294729548\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 37/100\n",
      "train_loss: -0.20263603189741503\n",
      "Validation loss decreased (-0.188383 --> -0.197409).  Saving model ...\n",
      "Epoch 38/100\n",
      "train_loss: -0.1967856143142136\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 39/100\n",
      "train_loss: -0.19567374052576053\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 40/100\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_clusters):\n",
    "\n",
    "    index = list(clusters[month-1] == i)\n",
    "    sub_series = series[index]\n",
    "    \n",
    "    test_data = sub_series[:, -168*2:].T\n",
    "    train_data = sub_series[:, :-168].T\n",
    "    \n",
    "    data_start = (train_data != 0).argmax(axis=0)\n",
    "    total_time = sub_series.shape[1]\n",
    "    num_series = sub_series.shape[0]\n",
    "    \n",
    "    window_size = 192\n",
    "    stride_size = 24\n",
    "    \n",
    "    # prepare data\n",
    "    cov = covariates.copy()\n",
    "    train_x_input, train_v_input, train_label = prep_data(train_data, cov, data_start, window_size, stride_size, num_covariates, num_series, total_time)\n",
    "    cov = covariates.copy()\n",
    "    test_x_input, test_v_input, test_label = prep_data(test_data, cov, data_start, window_size, stride_size, num_covariates, num_series, total_time, train=False)\n",
    "    \n",
    "    # params\n",
    "    json_path = os.path.join(path, 'forecasting', 'deepar', 'params24.json')\n",
    "    params = utils.Params(json_path)\n",
    "    \n",
    "    params.num_class = np.sum(index)\n",
    "    params.relative_metrics = False\n",
    "    params.sampling = False\n",
    "    params.one_step = True\n",
    "    \n",
    "    # use GPU if available\n",
    "    cuda_exist = torch.cuda.is_available()\n",
    "    \n",
    "    # Set random seeds for reproducible experiments if necessary\n",
    "    if cuda_exist:\n",
    "        params.device = torch.device('cuda')\n",
    "        # torch.cuda.manual_seed(240)\n",
    "        model = net.Net(params).cuda()\n",
    "    else:\n",
    "        params.device = torch.device('cpu')\n",
    "        # torch.manual_seed(230)\n",
    "        model = net.Net(params)\n",
    "    \n",
    "    # split train and valid\n",
    "    val_size = 0.2\n",
    "    num_train = len(train_x_input)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(val_size * num_train))\n",
    "    train_idx, val_idx = indices[split:], indices[:split]\n",
    "    \n",
    "    # dataset\n",
    "    train_set = TrainDataset(train_x_input[train_idx], train_label[train_idx])\n",
    "    val_set = TrainDataset(train_x_input[val_idx], train_label[val_idx])\n",
    "    test_set = TestDataset(test_x_input, test_v_input, test_label)\n",
    "    \n",
    "    # sampler\n",
    "    train_sampler = WeightedSampler(train_v_input[train_idx]) # Use weighted sampler instead of random sampler\n",
    "    val_sampler = WeightedSampler(train_v_input[val_idx])\n",
    "    \n",
    "    # loader\n",
    "    train_loader = DataLoader(train_set, batch_size=params.batch_size, sampler=train_sampler, num_workers=16)\n",
    "    val_loader = DataLoader(val_set, batch_size=256, sampler=val_sampler, num_workers=16)\n",
    "    test_loader = DataLoader(test_set, batch_size=params.predict_batch, sampler=RandomSampler(test_set), num_workers=16)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "    loss_fn = net.loss_fn\n",
    "    \n",
    "    restore_file = None\n",
    "    train_and_evaluate(model,\n",
    "                       train_loader,\n",
    "                       val_loader,\n",
    "                       test_loader,\n",
    "                       optimizer,\n",
    "                       loss_fn,\n",
    "                       params,\n",
    "                       restore_file)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net.Net(params)\n",
    "model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "test_metrics = evaluate(model, loss_fn, test_loader, params, params.sampling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
