{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy import stats\n",
    "import utils\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "\n",
    "import model.net as net\n",
    "from dataloader import *\n",
    "from train import train_and_evaluate\n",
    "\n",
    "months = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 918/918 [00:27<00:00, 32.90it/s]\n"
     ]
    }
   ],
   "source": [
    "data_set = 'Irish_2010'\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "\n",
    "data = get_data(path, data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = 1\n",
    "n_clusters = 2\n",
    "method = 'hierarchical/euclidean'\n",
    "\n",
    "path_cluster = os.path.join(path, 'result', data_set, 'clustering', 'point', method, f'n_clusters_{n_clusters}.csv')\n",
    "clusters = pd.read_csv(path_cluster, header=None)\n",
    "path_data = os.path.join(path, 'data', 'deepar')\n",
    "\n",
    "series = data[:, month-1, :months[month-1]*24]\n",
    "\n",
    "weather = get_weather(path, data_set, month)\n",
    "week = get_dow(data_set, month)\n",
    "day = get_hod(month)\n",
    "\n",
    "num_covariates = 4\n",
    "covariates = np.zeros((num_covariates, len(series[0])))\n",
    "covariates[1] = stats.zscore(weather)\n",
    "covariates[2] = stats.zscore(week)\n",
    "covariates[3] = stats.zscore(day)\n",
    "covariates = covariates.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training and evaluation\n",
      "Epoch 1/100\n",
      "train_loss: 1.0702459812164307\n",
      "train_loss: 0.6449604034423828\n",
      "train_loss: 0.6650168100992838\n",
      "train_loss: 0.6394361654917399\n",
      "train_loss: 0.639016588528951\n",
      "train_loss: 0.6191946665445963\n",
      "train_loss: 0.597570021947225\n",
      "train_loss: 0.5824446280797323\n",
      "train_loss: 0.5575777689615885\n",
      "train_loss: 0.5133227904637655\n",
      "train_loss: 0.5068854093551636\n",
      "train_loss: 0.46700795491536456\n",
      "train_loss: 0.45857715606689453\n",
      "train_loss: 0.438839594523112\n",
      "train_loss: 0.4471634229024251\n",
      "train_loss: 0.2879294753074646\n",
      "train_loss: 0.25182561079661053\n",
      "train_loss: 0.25548962752024335\n",
      "Current Best ND is: 0.64061\n",
      "Epoch 2/100\n",
      "train_loss: 0.2063016096750895\n",
      "train_loss: 0.26644758383433026\n",
      "train_loss: 0.1849398414293925\n",
      "train_loss: 0.12452889482180278\n",
      "train_loss: 0.18259602785110474\n",
      "train_loss: 0.20975236097971597\n",
      "train_loss: 0.127494345108668\n",
      "train_loss: 0.07399355371793111\n",
      "train_loss: 0.1098603109518687\n",
      "train_loss: 0.09642035762468974\n",
      "train_loss: 0.08278435468673706\n",
      "train_loss: 0.04998560746510824\n",
      "train_loss: 0.026537003616491955\n",
      "train_loss: 0.1130336324373881\n",
      "train_loss: 0.16543071468671164\n",
      "train_loss: 0.07787576814492543\n",
      "train_loss: 0.08353301882743835\n",
      "train_loss: -0.034361605842908226\n",
      "Current Best ND is: 0.64061\n",
      "Epoch 3/100\n",
      "train_loss: 0.0006635977576176325\n",
      "train_loss: 0.04754303892453512\n",
      "train_loss: 0.07789589464664459\n",
      "train_loss: -0.02610648423433304\n",
      "train_loss: 0.004700444017847379\n",
      "train_loss: -0.06112138430277506\n",
      "train_loss: 0.08066672583421071\n",
      "train_loss: -0.06104404727617899\n",
      "train_loss: 0.015322177360455195\n",
      "train_loss: 0.016628127545118332\n",
      "train_loss: 0.03400410215059916\n",
      "train_loss: -0.05031006038188934\n",
      "train_loss: -0.0395416592558225\n",
      "train_loss: -0.09215742349624634\n",
      "train_loss: -0.012811344116926193\n",
      "train_loss: -0.11495301127433777\n",
      "train_loss: -0.0413990393280983\n",
      "train_loss: 0.0006073042750358582\n",
      "Current Best ND is: 0.64061\n",
      "Epoch 4/100\n",
      "train_loss: 0.038226328790187836\n",
      "train_loss: -0.00336618193735679\n",
      "train_loss: -0.06326935191949208\n",
      "train_loss: -0.047822182377179466\n",
      "train_loss: -0.04311118026574453\n",
      "train_loss: -0.0833543340365092\n",
      "train_loss: 0.02018124982714653\n",
      "train_loss: -0.08621672789255778\n",
      "train_loss: -0.11877446373303731\n",
      "train_loss: -0.11837884783744812\n",
      "train_loss: -0.09971928596496582\n",
      "train_loss: -0.09739116827646892\n",
      "train_loss: -0.035146827499071755\n",
      "train_loss: -0.15411677956581116\n",
      "train_loss: -0.03843674063682556\n",
      "train_loss: -0.09731630484263103\n",
      "train_loss: -0.07905153930187225\n",
      "train_loss: -0.08678191900253296\n",
      "Current Best ND is: 0.64061\n",
      "Epoch 5/100\n",
      "train_loss: -0.14667837818463644\n",
      "train_loss: -0.07405919830004375\n",
      "train_loss: -0.2041017214457194\n",
      "train_loss: -0.09001097083091736\n",
      "train_loss: -0.06288878122965495\n",
      "train_loss: -0.10616795221964519\n",
      "train_loss: -0.11915422479311626\n",
      "train_loss: -0.11758548021316528\n",
      "train_loss: -0.07493147253990173\n",
      "train_loss: -0.13377301891644797\n",
      "train_loss: -0.1036117176214854\n",
      "train_loss: -0.09589962164560954\n",
      "train_loss: -0.16774932543436685\n",
      "train_loss: -0.11887555321057637\n",
      "train_loss: -0.22558128833770752\n",
      "train_loss: -0.132347305615743\n",
      "train_loss: -0.0869120458761851\n",
      "train_loss: -0.21897409359614053\n",
      "Current Best ND is: 0.64061\n",
      "Epoch 6/100\n",
      "train_loss: -0.021343839665253956\n",
      "train_loss: -0.12837469577789307\n",
      "train_loss: -0.07291783392429352\n",
      "train_loss: -0.07356366515159607\n",
      "train_loss: -0.13515125711758932\n",
      "train_loss: -0.16129917899767557\n",
      "train_loss: -0.07332196334997813\n",
      "train_loss: -0.16855873664220175\n",
      "train_loss: -0.14966055750846863\n",
      "train_loss: -0.1785354216893514\n",
      "train_loss: -0.18097823858261108\n",
      "train_loss: -0.09706275661786397\n",
      "train_loss: -0.21823330720265707\n",
      "train_loss: -0.12971660494804382\n",
      "train_loss: -0.10242098569869995\n",
      "train_loss: -0.1545071005821228\n",
      "train_loss: -0.1100884477297465\n",
      "train_loss: -0.07709492246309917\n",
      "Current Best ND is: 0.64061\n",
      "Epoch 7/100\n",
      "train_loss: -0.21975078185399374\n",
      "train_loss: -0.18434820572535196\n",
      "train_loss: -0.04831609129905701\n",
      "train_loss: -0.15836955110232034\n",
      "train_loss: -0.11857311924298604\n",
      "train_loss: -0.17473320166269937\n",
      "train_loss: -0.23582200209299722\n",
      "train_loss: -0.19454161326090494\n",
      "train_loss: -0.09779158234596252\n",
      "train_loss: -0.13581245144208273\n",
      "train_loss: -0.17871532837549844\n",
      "train_loss: -0.21620859702428183\n",
      "train_loss: -0.10723499457041423\n",
      "train_loss: -0.18627707163492838\n",
      "train_loss: -0.2878624399503072\n",
      "train_loss: -0.1902857025464376\n",
      "train_loss: -0.22542903820673624\n",
      "train_loss: -0.16799914836883545\n",
      "Current Best ND is: 0.64061\n",
      "Epoch 8/100\n",
      "train_loss: -0.20957845449447632\n",
      "train_loss: -0.24037124713261923\n",
      "train_loss: -0.22005275885264078\n",
      "train_loss: -0.22617900371551514\n",
      "train_loss: -0.2540140748023987\n",
      "train_loss: -0.1911004384358724\n",
      "train_loss: -0.1700697143872579\n",
      "train_loss: -0.1430628498395284\n",
      "train_loss: -0.2635982433954875\n",
      "train_loss: -0.17012755076090494\n",
      "train_loss: -0.23867950836817423\n",
      "train_loss: -0.20734133323033652\n",
      "train_loss: -0.14507315556208292\n",
      "train_loss: -0.13948593537012735\n",
      "train_loss: -0.2940547664960225\n",
      "train_loss: -0.18575110038121542\n",
      "train_loss: -0.05778701603412628\n",
      "train_loss: -0.2078266739845276\n",
      "Current Best ND is: 0.63161\n",
      "Epoch 9/100\n",
      "train_loss: -0.17188803354899088\n",
      "train_loss: -0.18864727020263672\n",
      "train_loss: -0.2538827657699585\n",
      "train_loss: -0.2526129086812337\n",
      "train_loss: -0.21848533550898233\n",
      "train_loss: -0.15044212341308594\n",
      "train_loss: -0.167619526386261\n",
      "train_loss: -0.20094982782999674\n",
      "train_loss: -0.2777620355288188\n",
      "train_loss: -0.1741669774055481\n",
      "train_loss: -0.14665801326433817\n",
      "train_loss: -0.25126926104227704\n",
      "train_loss: -0.22956283887227377\n",
      "train_loss: -0.2573920488357544\n",
      "train_loss: -0.16561796267827353\n",
      "train_loss: -0.24241129557291666\n",
      "train_loss: -0.24176722764968872\n",
      "train_loss: -0.2319905161857605\n",
      "Current Best ND is: 0.63161\n",
      "Epoch 10/100\n",
      "train_loss: -0.27247337500254315\n",
      "train_loss: -0.1663291354974111\n",
      "train_loss: -0.24145897229512533\n",
      "train_loss: -0.265872319539388\n",
      "train_loss: -0.2874446113904317\n",
      "train_loss: -0.35084299246470135\n",
      "train_loss: -0.1488445003827413\n",
      "train_loss: -0.2213916778564453\n",
      "train_loss: -0.21480673551559448\n",
      "train_loss: -0.19909930229187012\n",
      "train_loss: -0.2363276481628418\n",
      "train_loss: -0.202707568804423\n",
      "train_loss: -0.2483979066212972\n",
      "train_loss: -0.22214688857396445\n",
      "train_loss: -0.2482561469078064\n",
      "train_loss: -0.22884505987167358\n",
      "train_loss: -0.32128016153971356\n",
      "train_loss: -0.17761937777201334\n",
      "Current Best ND is: 0.63161\n",
      "Epoch 11/100\n",
      "train_loss: -0.214316725730896\n",
      "train_loss: -0.2761903206507365\n",
      "train_loss: -0.2793068289756775\n",
      "train_loss: -0.2024937868118286\n",
      "train_loss: -0.2017938494682312\n",
      "train_loss: -0.36864415804545086\n",
      "train_loss: -0.19152446587880453\n",
      "train_loss: -0.2655078371365865\n",
      "train_loss: -0.3230318824450175\n",
      "train_loss: -0.2719104290008545\n",
      "train_loss: -0.2779863476753235\n",
      "train_loss: -0.26833699146906537\n",
      "train_loss: -0.2916344205538432\n",
      "train_loss: -0.26752116282780963\n",
      "train_loss: -0.2932223677635193\n",
      "train_loss: -0.3019714554150899\n",
      "train_loss: -0.24423609177271524\n",
      "train_loss: -0.23422823349634805\n",
      "Current Best ND is: 0.63161\n",
      "Epoch 12/100\n",
      "train_loss: -0.33851027488708496\n",
      "train_loss: -0.2433221141497294\n",
      "train_loss: -0.2706604599952698\n",
      "train_loss: -0.2490027149518331\n",
      "train_loss: -0.1988552212715149\n",
      "train_loss: -0.2558026115099589\n",
      "train_loss: -0.23627211650212607\n",
      "train_loss: -0.22139414151509604\n",
      "train_loss: -0.1979147990544637\n",
      "train_loss: -0.2441675861676534\n",
      "train_loss: -0.32052431503931683\n",
      "train_loss: -0.2675895094871521\n",
      "train_loss: -0.24015019337336221\n",
      "train_loss: -0.22205398480097452\n",
      "train_loss: -0.3337483803431193\n",
      "train_loss: -0.25462549924850464\n",
      "train_loss: -0.3199143409729004\n",
      "train_loss: -0.2656214237213135\n",
      "Current Best ND is: 0.63161\n",
      "Epoch 13/100\n",
      "train_loss: -0.2534262339274089\n",
      "train_loss: -0.23839219411214194\n",
      "train_loss: -0.31951568524042767\n",
      "train_loss: -0.2719738284746806\n",
      "train_loss: -0.2701434890429179\n",
      "train_loss: -0.22719303766886392\n",
      "train_loss: -0.2571297486623128\n",
      "train_loss: -0.3536011377970378\n",
      "train_loss: -0.23637163639068604\n",
      "train_loss: -0.2605937123298645\n",
      "train_loss: -0.22450989484786987\n",
      "train_loss: -0.2526084979375203\n",
      "train_loss: -0.19881687561670938\n",
      "train_loss: -0.2649957537651062\n",
      "train_loss: -0.234930157661438\n",
      "train_loss: -0.3541967074076335\n",
      "train_loss: -0.2608917752901713\n",
      "train_loss: -0.28841928641001385\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 14/100\n",
      "train_loss: -0.29130218426386517\n",
      "train_loss: -0.27070152759552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: -0.27539946635564166\n",
      "train_loss: -0.34889670213063556\n",
      "train_loss: -0.3337411085764567\n",
      "train_loss: -0.20682334899902344\n",
      "train_loss: -0.3107630213101705\n",
      "train_loss: -0.31884392102559406\n",
      "train_loss: -0.2742966612180074\n",
      "train_loss: -0.17296463251113892\n",
      "train_loss: -0.2677127917607625\n",
      "train_loss: -0.26441073417663574\n",
      "train_loss: -0.3081008195877075\n",
      "train_loss: -0.32645291090011597\n",
      "train_loss: -0.2860971490542094\n",
      "train_loss: -0.26687244574228924\n",
      "train_loss: -0.23664226134618124\n",
      "train_loss: -0.30629334847132367\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 15/100\n",
      "train_loss: -0.2598416606585185\n",
      "train_loss: -0.25453827778498334\n",
      "train_loss: -0.33248021205266315\n",
      "train_loss: -0.3938799301783244\n",
      "train_loss: -0.2942809462547302\n",
      "train_loss: -0.271628479162852\n",
      "train_loss: -0.39035165309906006\n",
      "train_loss: -0.2718234658241272\n",
      "train_loss: -0.29577813545862836\n",
      "train_loss: -0.22974437475204468\n",
      "train_loss: -0.3228866060574849\n",
      "train_loss: -0.29054681460062665\n",
      "train_loss: -0.334042231241862\n",
      "train_loss: -0.28191723426183063\n",
      "train_loss: -0.30927640199661255\n",
      "train_loss: -0.3169653018315633\n",
      "train_loss: -0.296779990196228\n",
      "train_loss: -0.26168936491012573\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 16/100\n",
      "train_loss: -0.3457865317662557\n",
      "train_loss: -0.30571385224660236\n",
      "train_loss: -0.26629072427749634\n",
      "train_loss: -0.287641962369283\n",
      "train_loss: -0.39065225919087726\n",
      "train_loss: -0.3155967791875203\n",
      "train_loss: -0.27496469020843506\n",
      "train_loss: -0.3319864471753438\n",
      "train_loss: -0.32874486843744916\n",
      "train_loss: -0.35083842277526855\n",
      "train_loss: -0.265558660030365\n",
      "train_loss: -0.3478558858235677\n",
      "train_loss: -0.2924009362856547\n",
      "train_loss: -0.24325879414876303\n",
      "train_loss: -0.26381484667460126\n",
      "train_loss: -0.29567909240722656\n",
      "train_loss: -0.2145135998725891\n",
      "train_loss: -0.4144999186197917\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 17/100\n",
      "train_loss: -0.331570307413737\n",
      "train_loss: -0.27150193850199383\n",
      "train_loss: -0.32106101512908936\n",
      "train_loss: -0.23591655492782593\n",
      "train_loss: -0.35471920172373456\n",
      "train_loss: -0.24719919761021933\n",
      "train_loss: -0.22604556878407797\n",
      "train_loss: -0.43019044399261475\n",
      "train_loss: -0.2887478470802307\n",
      "train_loss: -0.25390368700027466\n",
      "train_loss: -0.2799980839093526\n",
      "train_loss: -0.322498897711436\n",
      "train_loss: -0.3482629458109538\n",
      "train_loss: -0.34982840220133465\n",
      "train_loss: -0.34215815862019855\n",
      "train_loss: -0.4123829205830892\n",
      "train_loss: -0.29660916328430176\n",
      "train_loss: -0.35432342688242596\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 18/100\n",
      "train_loss: -0.36416248480478924\n",
      "train_loss: -0.3157977859179179\n",
      "train_loss: -0.5114227533340454\n",
      "train_loss: -0.2767969568570455\n",
      "train_loss: -0.2708363135655721\n",
      "train_loss: -0.3463866710662842\n",
      "train_loss: -0.32843772570292157\n",
      "train_loss: -0.3307211995124817\n",
      "train_loss: -0.2964520851771037\n",
      "train_loss: -0.26397842168807983\n",
      "train_loss: -0.32522112131118774\n",
      "train_loss: -0.3149364988009135\n",
      "train_loss: -0.26953889926274616\n",
      "train_loss: -0.27220352490743\n",
      "train_loss: -0.3393416404724121\n",
      "train_loss: -0.27409132321675617\n",
      "train_loss: -0.2966581384340922\n",
      "train_loss: -0.2665789723396301\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 19/100\n",
      "train_loss: -0.36748655637105304\n",
      "train_loss: -0.40026764074961346\n",
      "train_loss: -0.3505204916000366\n",
      "train_loss: -0.33639955520629883\n",
      "train_loss: -0.3327040672302246\n",
      "train_loss: -0.4058388868967692\n",
      "train_loss: -0.3650796016057332\n",
      "train_loss: -0.34244608879089355\n",
      "train_loss: -0.2808901071548462\n",
      "train_loss: -0.2604290246963501\n",
      "train_loss: -0.3178955515225728\n",
      "train_loss: -0.2977496385574341\n",
      "train_loss: -0.3231985966364543\n",
      "train_loss: -0.3278810183207194\n",
      "train_loss: -0.2591628432273865\n",
      "train_loss: -0.33500762780507404\n",
      "train_loss: -0.3301038344701131\n",
      "train_loss: -0.3420497576395671\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 20/100\n",
      "train_loss: -0.3417791525522868\n",
      "train_loss: -0.2917741934458415\n",
      "train_loss: -0.4248984257380168\n",
      "train_loss: -0.2955508629480998\n",
      "train_loss: -0.27847711245218915\n",
      "train_loss: -0.36356472969055176\n",
      "train_loss: -0.34404226144154865\n",
      "train_loss: -0.35242148240407306\n",
      "train_loss: -0.24939177433649698\n",
      "train_loss: -0.37274642785390216\n",
      "train_loss: -0.29557643334070843\n",
      "train_loss: -0.3535756667455037\n",
      "train_loss: -0.42737750212351483\n",
      "train_loss: -0.32313795884450275\n",
      "train_loss: -0.3060686985651652\n",
      "train_loss: -0.41678380966186523\n",
      "train_loss: -0.38862041632334393\n",
      "train_loss: -0.4139695167541504\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 21/100\n",
      "train_loss: -0.3228458563486735\n",
      "train_loss: -0.3876719077428182\n",
      "train_loss: -0.3126639525095622\n",
      "train_loss: -0.2746856411298116\n",
      "train_loss: -0.3324073553085327\n",
      "train_loss: -0.4945462942123413\n",
      "train_loss: -0.3772096633911133\n",
      "train_loss: -0.324871301651001\n",
      "train_loss: -0.4113758405049642\n",
      "train_loss: -0.2989742159843445\n",
      "train_loss: -0.3062039812405904\n",
      "train_loss: -0.38843055566151935\n",
      "train_loss: -0.3894945780436198\n",
      "train_loss: -0.35702967643737793\n",
      "train_loss: -0.4110960563023885\n",
      "train_loss: -0.32859452565511066\n",
      "train_loss: -0.398006796836853\n",
      "train_loss: -0.45231016476949054\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 22/100\n",
      "train_loss: -0.2964435815811157\n",
      "train_loss: -0.3624570369720459\n",
      "train_loss: -0.3779810667037964\n",
      "train_loss: -0.22929092248280844\n",
      "train_loss: -0.259404460589091\n",
      "train_loss: -0.3993537425994873\n",
      "train_loss: -0.4048378070195516\n",
      "train_loss: -0.3837110598882039\n",
      "train_loss: -0.3098536729812622\n",
      "train_loss: -0.39376382033030194\n",
      "train_loss: -0.36842016379038495\n",
      "train_loss: -0.31287946303685504\n",
      "train_loss: -0.38172562917073566\n",
      "train_loss: -0.41160809993743896\n",
      "train_loss: -0.33687782287597656\n",
      "train_loss: -0.39245720704396564\n",
      "train_loss: -0.3691709836324056\n",
      "train_loss: -0.3576269547144572\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 23/100\n",
      "train_loss: -0.3428143262863159\n",
      "train_loss: -0.4238537549972534\n",
      "train_loss: -0.3465389410654704\n",
      "train_loss: -0.3414478699366252\n",
      "train_loss: -0.41552992661794025\n",
      "train_loss: -0.4264358679453532\n",
      "train_loss: -0.34905465443929035\n",
      "train_loss: -0.35336939493815106\n",
      "train_loss: -0.3180363178253174\n",
      "train_loss: -0.32845987876256305\n",
      "train_loss: -0.38846953709920246\n",
      "train_loss: -0.38017602761586505\n",
      "train_loss: -0.368442177772522\n",
      "train_loss: -0.30355580647786456\n",
      "train_loss: -0.3716793457667033\n",
      "train_loss: -0.449791153271993\n",
      "train_loss: -0.32242294152577716\n",
      "train_loss: -0.35841349760691327\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 24/100\n",
      "train_loss: -0.5057787895202637\n",
      "train_loss: -0.4486049811045329\n",
      "train_loss: -0.39275312423706055\n",
      "train_loss: -0.43094674746195477\n",
      "train_loss: -0.3202883005142212\n",
      "train_loss: -0.32885785897572833\n",
      "train_loss: -0.3518434365590413\n",
      "train_loss: -0.2753253976504008\n",
      "train_loss: -0.308258593082428\n",
      "train_loss: -0.3114246924718221\n",
      "train_loss: -0.40868882338205975\n",
      "train_loss: -0.39439864953358966\n",
      "train_loss: -0.26848336060841876\n",
      "train_loss: -0.3479491074879964\n",
      "train_loss: -0.4166719913482666\n",
      "train_loss: -0.39600030581156415\n",
      "train_loss: -0.45658167203267414\n",
      "train_loss: -0.3553617795308431\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 25/100\n",
      "train_loss: -0.4791831970214844\n",
      "train_loss: -0.38361767927805585\n",
      "train_loss: -0.3479651212692261\n",
      "train_loss: -0.37501569588979083\n",
      "train_loss: -0.36092456181844074\n",
      "train_loss: -0.3384169340133667\n",
      "train_loss: -0.32534076770146686\n",
      "train_loss: -0.4000887870788574\n",
      "train_loss: -0.33968857924143475\n",
      "train_loss: -0.4167265097300212\n",
      "train_loss: -0.3558310270309448\n",
      "train_loss: -0.3901124397913615\n",
      "train_loss: -0.326610803604126\n",
      "train_loss: -0.5197097460428873\n",
      "train_loss: -0.30794010559717816\n",
      "train_loss: -0.3701467514038086\n",
      "train_loss: -0.34477460384368896\n",
      "train_loss: -0.28476760784784955\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 26/100\n",
      "train_loss: -0.3738851547241211\n",
      "train_loss: -0.4672681490580241\n",
      "train_loss: -0.41958566506703693\n",
      "train_loss: -0.4122971296310425\n",
      "train_loss: -0.338270902633667\n",
      "train_loss: -0.42217286427815753\n",
      "train_loss: -0.4099303086598714\n",
      "train_loss: -0.35954233010609943\n",
      "train_loss: -0.3327428897221883\n",
      "train_loss: -0.35285377502441406\n",
      "train_loss: -0.377848744392395\n",
      "train_loss: -0.395915945370992\n",
      "train_loss: -0.293609619140625\n",
      "train_loss: -0.296492060025533\n",
      "train_loss: -0.4118604262669881\n",
      "train_loss: -0.3969961404800415\n",
      "train_loss: -0.3270765542984009\n",
      "train_loss: -0.3677521546681722\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 27/100\n",
      "train_loss: -0.4280913670857747\n",
      "train_loss: -0.38299206892649335\n",
      "train_loss: -0.3833484649658203\n",
      "train_loss: -0.37625869115193683\n",
      "train_loss: -0.41976877053578693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: -0.49101348718007404\n",
      "train_loss: -0.4422581195831299\n",
      "train_loss: -0.39866940180460614\n",
      "train_loss: -0.3487079938252767\n",
      "train_loss: -0.362565557161967\n",
      "train_loss: -0.3070661822954814\n",
      "train_loss: -0.3743214209874471\n",
      "train_loss: -0.30561800797780353\n",
      "train_loss: -0.39610175291697186\n",
      "train_loss: -0.3550885121027629\n",
      "train_loss: -0.35532716910044354\n",
      "train_loss: -0.3973332643508911\n",
      "train_loss: -0.4144953489303589\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 28/100\n",
      "train_loss: -0.32663458585739136\n",
      "train_loss: -0.43190352121988934\n",
      "train_loss: -0.3609148661295573\n",
      "train_loss: -0.38879791895548504\n",
      "train_loss: -0.3407641251881917\n",
      "train_loss: -0.38636688391367596\n",
      "train_loss: -0.43711618582407635\n",
      "train_loss: -0.3913713296254476\n",
      "train_loss: -0.3707411289215088\n",
      "train_loss: -0.3624272346496582\n",
      "train_loss: -0.3533920844395955\n",
      "train_loss: -0.4011656443277995\n",
      "train_loss: -0.4184991518656413\n",
      "train_loss: -0.3745376666386922\n",
      "train_loss: -0.37162025769551593\n",
      "train_loss: -0.40406835079193115\n",
      "train_loss: -0.3773762385050456\n",
      "train_loss: -0.45662736892700195\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 29/100\n",
      "train_loss: -0.40917325019836426\n",
      "train_loss: -0.3419971466064453\n",
      "train_loss: -0.30109065771102905\n",
      "train_loss: -0.37808120250701904\n",
      "train_loss: -0.4841732184092204\n",
      "train_loss: -0.38842658201853436\n",
      "train_loss: -0.34996581077575684\n",
      "train_loss: -0.41446252663930255\n",
      "train_loss: -0.43632086118062335\n",
      "train_loss: -0.37270065148671466\n",
      "train_loss: -0.34100182851155597\n",
      "train_loss: -0.49139920870463055\n",
      "train_loss: -0.44416606426239014\n",
      "train_loss: -0.4632660150527954\n",
      "train_loss: -0.38091325759887695\n",
      "train_loss: -0.48701830705006915\n",
      "train_loss: -0.36423687140146893\n",
      "train_loss: -0.39611788590749103\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 30/100\n",
      "train_loss: -0.43952163060506183\n",
      "train_loss: -0.4296436707178752\n",
      "train_loss: -0.42731328805287677\n",
      "train_loss: -0.4269488255182902\n",
      "train_loss: -0.4415295521418254\n",
      "train_loss: -0.39816168944040936\n",
      "train_loss: -0.40253619352976483\n",
      "train_loss: -0.4100353717803955\n",
      "train_loss: -0.40323440233866376\n",
      "train_loss: -0.43798621495564777\n",
      "train_loss: -0.35255030790964764\n",
      "train_loss: -0.36327898502349854\n",
      "train_loss: -0.4438209533691406\n",
      "train_loss: -0.39173702398935956\n",
      "train_loss: -0.3592788775761922\n",
      "train_loss: -0.42055078347524005\n",
      "train_loss: -0.4198841651280721\n",
      "train_loss: -0.4352149963378906\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 31/100\n",
      "train_loss: -0.407502015431722\n",
      "train_loss: -0.4149659077326457\n",
      "train_loss: -0.3608893156051636\n",
      "train_loss: -0.42179008324941\n",
      "train_loss: -0.5398495197296143\n",
      "train_loss: -0.48433295885721844\n",
      "train_loss: -0.4240315755208333\n",
      "train_loss: -0.4885369539260864\n",
      "train_loss: -0.43218715985616046\n",
      "train_loss: -0.43474090099334717\n",
      "train_loss: -0.3586889108022054\n",
      "train_loss: -0.418136994043986\n",
      "train_loss: -0.4165659348169963\n",
      "train_loss: -0.4462917248408\n",
      "train_loss: -0.3781721591949463\n",
      "train_loss: -0.5041342576344808\n",
      "train_loss: -0.3146584828694661\n",
      "train_loss: -0.4615782896677653\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 32/100\n",
      "train_loss: -0.40271008014678955\n",
      "train_loss: -0.38696618874867755\n",
      "train_loss: -0.43798311551411945\n",
      "train_loss: -0.39992427825927734\n",
      "train_loss: -0.4066973129908244\n",
      "train_loss: -0.4555123647054036\n",
      "train_loss: -0.39776015281677246\n",
      "train_loss: -0.4184376001358032\n",
      "train_loss: -0.34263678391774494\n",
      "train_loss: -0.47397077083587646\n",
      "train_loss: -0.38662540912628174\n",
      "train_loss: -0.45213890075683594\n",
      "train_loss: -0.35562896728515625\n",
      "train_loss: -0.40674559275309247\n",
      "train_loss: -0.49713702996571857\n",
      "train_loss: -0.4565625588099162\n",
      "train_loss: -0.42416834831237793\n",
      "train_loss: -0.4401543935139974\n",
      "Current Best ND is: 0.62594\n",
      "Epoch 33/100\n",
      "train_loss: -0.38984179496765137\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a4d52ec53868>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m                        \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                        \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                        restore_file)\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ProbabilisticLoadForecasting/forecasting/deepar/train.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_loader, test_loader, optimizer, loss_fn, params, restore_file)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         loss_summary[epoch * train_len:(epoch + 1) * train_len] = train(model, optimizer, loss_fn, train_loader,\n\u001b[0;32m--> 107\u001b[0;31m                                                                         test_loader, params, epoch)\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;31m# evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mtest_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ProbabilisticLoadForecasting/forecasting/deepar/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loss_fn, train_loader, test_loader, params, epoch)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_window\u001b[0m  \u001b[0;31m# loss per timestep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(n_clusters):\n",
    "\n",
    "    index = list(clusters[month-1] == i)\n",
    "    sub_series = series[index]\n",
    "    \n",
    "    test_data = sub_series[:, -168*2:].T\n",
    "    train_data = sub_series[:, :-168].T\n",
    "    \n",
    "    data_start = (train_data != 0).argmax(axis=0)\n",
    "    total_time = sub_series.shape[1]\n",
    "    num_series = sub_series.shape[0]\n",
    "    \n",
    "    window_size = 192\n",
    "    stride_size = 24\n",
    "    \n",
    "    # prepare data\n",
    "    cov = covariates.copy()\n",
    "    train_x_input, train_v_input, train_label = prep_data(train_data, cov, data_start, window_size, stride_size, num_covariates, num_series, total_time)\n",
    "    cov = covariates.copy()\n",
    "    test_x_input, test_v_input, test_label = prep_data(test_data, cov, data_start, window_size, stride_size, num_covariates, num_series, total_time, train=False)\n",
    "    \n",
    "    # params\n",
    "    json_path = os.path.join(path, 'forecasting', 'deepar', 'params.json')\n",
    "    params = utils.Params(json_path)\n",
    "    \n",
    "    params.relative_metrics = False\n",
    "    params.sampling = False\n",
    "    \n",
    "    # use GPU if available\n",
    "    cuda_exist = torch.cuda.is_available()\n",
    "    # Set random seeds for reproducible experiments if necessary\n",
    "    if cuda_exist:\n",
    "        params.device = torch.device('cuda')\n",
    "        # torch.cuda.manual_seed(240)\n",
    "        model = net.Net(params).cuda()\n",
    "    else:\n",
    "        params.device = torch.device('cpu')\n",
    "        # torch.manual_seed(230)\n",
    "        model = net.Net(params)\n",
    "    \n",
    "    train_set = TrainDataset(train_x_input, train_label)\n",
    "    test_set = TestDataset(test_x_input, test_v_input, test_label)\n",
    "    sampler = WeightedSampler(train_v_input) # Use weighted sampler instead of random sampler\n",
    "    train_loader = DataLoader(train_set, batch_size=params.batch_size, sampler=sampler, num_workers=4)\n",
    "    test_loader = DataLoader(test_set, batch_size=params.predict_batch, sampler=RandomSampler(test_set), num_workers=4)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "    loss_fn = net.loss_fn\n",
    "    \n",
    "    restore_file = None\n",
    "    train_and_evaluate(model,\n",
    "                       train_loader,\n",
    "                       test_loader,\n",
    "                       optimizer,\n",
    "                       loss_fn,\n",
    "                       params,\n",
    "                       restore_file)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
