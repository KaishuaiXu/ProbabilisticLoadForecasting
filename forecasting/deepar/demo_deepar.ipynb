{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy import stats\n",
    "import utils\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "\n",
    "import model.net as net\n",
    "from dataloader import *\n",
    "from train import train_and_evaluate\n",
    "\n",
    "months = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 918/918 [00:29<00:00, 30.23it/s]\n"
     ]
    }
   ],
   "source": [
    "data_set = 'Irish_2010'\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "\n",
    "data = get_data(path, data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = 1\n",
    "n_clusters = 2\n",
    "method = 'hierarchical/euclidean'\n",
    "\n",
    "path_cluster = os.path.join(path, 'result', data_set, 'clustering', 'point', method, f'n_clusters_{n_clusters}.csv')\n",
    "clusters = pd.read_csv(path_cluster, header=None)\n",
    "path_data = os.path.join(path, 'data', 'deepar')\n",
    "\n",
    "series = data[:, month-1, :months[month-1]*24]\n",
    "\n",
    "weather = get_weather(path, data_set, month)\n",
    "week = get_dow(data_set, month)\n",
    "day = get_hod(month)\n",
    "\n",
    "num_covariates = 4\n",
    "covariates = np.zeros((num_covariates, len(series[0])))\n",
    "covariates[1] = stats.zscore(weather)\n",
    "covariates[2] = stats.zscore(week)\n",
    "covariates[3] = stats.zscore(day)\n",
    "covariates = covariates.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training\n",
      "Epoch 1/20\n",
      "train_loss: 1.129333766959828\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_clusters):\n",
    "\n",
    "    index = list(clusters[month-1] == i)\n",
    "    sub_series = series[index]\n",
    "    \n",
    "    test_data = sub_series[:, -168*2:].T\n",
    "    train_data = sub_series[:, :-168].T\n",
    "    \n",
    "    data_start = (train_data != 0).argmax(axis=0)\n",
    "    total_time = sub_series.shape[1]\n",
    "    num_series = sub_series.shape[0]\n",
    "    \n",
    "    window_size = 169\n",
    "    stride_size = 1\n",
    "    \n",
    "    # prepare data\n",
    "    cov = covariates.copy()\n",
    "    train_x_input, train_v_input, train_label = prep_data(train_data, cov, data_start, window_size, stride_size, num_covariates, num_series, total_time)\n",
    "    cov = covariates.copy()\n",
    "    test_x_input, test_v_input, test_label = prep_data(test_data, cov, data_start, window_size, stride_size, num_covariates, num_series, total_time, train=False)\n",
    "    \n",
    "    # params\n",
    "    json_path = os.path.join(path, 'forecasting', 'deepar', 'params1.json')\n",
    "    params = utils.Params(json_path)\n",
    "    \n",
    "    params.num_class = np.sum(index)\n",
    "    params.relative_metrics = False\n",
    "    params.sampling = False\n",
    "    \n",
    "    # use GPU if available\n",
    "    cuda_exist = torch.cuda.is_available()\n",
    "    # Set random seeds for reproducible experiments if necessary\n",
    "    if cuda_exist:\n",
    "        params.device = torch.device('cuda')\n",
    "        # torch.cuda.manual_seed(240)\n",
    "        model = net.Net(params).cuda()\n",
    "    else:\n",
    "        params.device = torch.device('cpu')\n",
    "        # torch.manual_seed(230)\n",
    "        model = net.Net(params)\n",
    "    \n",
    "    train_set = TrainDataset(train_x_input, train_label)\n",
    "    test_set = TestDataset(test_x_input, test_v_input, test_label)\n",
    "    sampler = WeightedSampler(train_v_input) # Use weighted sampler instead of random sampler\n",
    "    train_loader = DataLoader(train_set, batch_size=params.batch_size, sampler=sampler, num_workers=4)\n",
    "    test_loader = DataLoader(test_set, batch_size=params.predict_batch, sampler=RandomSampler(test_set), num_workers=4)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "    loss_fn = net.loss_fn\n",
    "    \n",
    "    restore_file = None\n",
    "    train_and_evaluate(model,\n",
    "                       train_loader,\n",
    "                       test_loader,\n",
    "                       optimizer,\n",
    "                       loss_fn,\n",
    "                       params,\n",
    "                       restore_file)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
