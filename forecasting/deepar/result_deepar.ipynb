{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy import stats\n",
    "import utils\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "\n",
    "import model.net as net\n",
    "from dataloader import *\n",
    "from train import train_and_evaluate\n",
    "from evaluate import evaluate\n",
    "\n",
    "months = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "\n",
    "data_set = 'London_2013'\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "\n",
    "data = get_data(path, data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qloss(y_true, y_pred):\n",
    "    q = np.array(range(1, 100))\n",
    "    tmp1 = (q / 100 - 1) * (y_true - y_pred)\n",
    "    tmp2 = q / 100 * (y_true - y_pred)\n",
    "    return np.mean(np.maximum(tmp1, tmp2))\n",
    "\n",
    "def ws(alpha, l, u, y_true):\n",
    "    total_ws = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] >= l[i] and y_true[i] <= u[i]:\n",
    "            ws = u[i] - l[i]\n",
    "            total_ws = total_ws + ws\n",
    "        if y_true[i] < l[i]:\n",
    "            ws = u[i] - l[i] + 2 * (l[i] - y_true[i]) / alpha\n",
    "            total_ws = total_ws + ws\n",
    "        if y_true[i] > u[i]:\n",
    "            ws = u[i] - l[i] + 2 * (y_true[i] - u[i]) / alpha\n",
    "            total_ws = total_ws + ws\n",
    "    return total_ws/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_times = 1\n",
    "error_qs = np.zeros((total_times, 12, 10))\n",
    "error_ws_90 = np.zeros((total_times, 12, 10))\n",
    "error_ws_50 = np.zeros((total_times, 12, 10))\n",
    "\n",
    "method = 'kmeans'\n",
    "\n",
    "for times in trange(1, total_times+1):\n",
    "    for month in trange(1, 13):\n",
    "        for n_clusters in range(1, 11):\n",
    "\n",
    "            path_cluster = os.path.join(path, 'result', data_set, 'clustering', 'point', method, f'n_clusters_{n_clusters}.csv')\n",
    "            clusters = pd.read_csv(path_cluster, header=None)\n",
    "\n",
    "            series = data[:, month-1, :months[month-1]*24].T.copy()\n",
    "\n",
    "            window_size = 192\n",
    "            stride_size = 24\n",
    "\n",
    "            total_time = series.shape[0]\n",
    "            num_series = series.shape[1]\n",
    "\n",
    "            weather = get_weather(path, data_set, month)\n",
    "            week = get_dow(data_set, month)\n",
    "            day = get_hod(month)\n",
    "\n",
    "            num_covariates = 4\n",
    "            covariates = np.zeros((num_covariates, len(series)))\n",
    "            covariates[1] = stats.zscore(weather)\n",
    "            covariates[2] = stats.zscore(week)\n",
    "            covariates[3] = stats.zscore(day)\n",
    "            cov_age = stats.zscore(np.arange(total_time))\n",
    "            covariates[0] = cov_age\n",
    "            covariates = covariates.T.copy()\n",
    "\n",
    "            # params\n",
    "            json_path = os.path.join(path, 'forecasting', 'deepar', 'params24.json')\n",
    "            params = utils.Params(json_path)\n",
    "\n",
    "            params.num_class = n_clusters\n",
    "            params.relative_metrics = False\n",
    "            params.sampling = False\n",
    "            params.one_step = True\n",
    "\n",
    "            model_dir = os.path.join(path, 'result', data_set, 'forecasting', 'deepar', f'times_{times}', method)\n",
    "            if not os.path.exists(model_dir):\n",
    "                os.makedirs(model_dir)\n",
    "            params.model_dir = os.path.join(model_dir, f'n_clusters_{n_clusters}_month_{month}.pth.tar')\n",
    "\n",
    "            # use GPU if available\n",
    "            cuda_exist = torch.cuda.is_available()\n",
    "\n",
    "            # Set random seeds for reproducible experiments if necessary\n",
    "            if cuda_exist:\n",
    "                params.device = torch.device('cuda')\n",
    "                # torch.cuda.manual_seed(240)\n",
    "                model = net.Net(params).cuda()\n",
    "            else:\n",
    "                params.device = torch.device('cpu')\n",
    "                # torch.manual_seed(230)\n",
    "                model = net.Net(params)\n",
    "\n",
    "            optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "            loss_fn = net.loss_fn\n",
    "\n",
    "            restore_file = None\n",
    "\n",
    "            model = net.Net(params)\n",
    "            utils.load_checkpoint(params.model_dir, model)\n",
    "\n",
    "            total_mu = []\n",
    "            total_sigma = []\n",
    "\n",
    "            for day in range(1, 8):\n",
    "                if day != 7:\n",
    "                    test_data = series[-168-(8-day)*24:-(8-day)*24+24, :].copy()\n",
    "                    cov = covariates[-168-(8-day)*24:-(8-day)*24+24, :].copy()\n",
    "                else:\n",
    "                    test_data = series[-192:, :].copy()\n",
    "                    cov = covariates[-192:, :].copy()\n",
    "\n",
    "                test_x_input, test_v_input, test_label = prep_data(test_data, cov, window_size, stride_size, num_covariates, num_series, clusters[month-1], train=False)\n",
    "                test_set = TestDataset(test_x_input, test_v_input, test_label)\n",
    "                test_loader = DataLoader(test_set, batch_size=params.predict_batch, sampler=RandomSampler(test_set), num_workers=4)\n",
    "\n",
    "                test_metrics, mu, sigma = evaluate(model, loss_fn, test_loader, params, sample=params.sampling)\n",
    "\n",
    "                day_mu = torch.zeros(24)\n",
    "                day_sigma = torch.zeros(24)\n",
    "                for i in range(len(mu)):\n",
    "                    day_mu = day_mu + torch.sum(mu[i], axis=0)\n",
    "                    day_sigma = day_sigma + torch.sum(sigma[i]**2, axis=0)\n",
    "                total_mu.append(day_mu.numpy())\n",
    "                total_sigma.append(torch.sqrt(day_sigma).numpy())\n",
    "\n",
    "            total_mu = np.array(total_mu).reshape(-1)\n",
    "            total_sigma = np.array(total_sigma).reshape(-1)\n",
    "\n",
    "            pred = np.zeros((168, 99))\n",
    "\n",
    "            for i in range(len(total_mu)):\n",
    "                tmp = np.random.normal(total_mu[i], total_sigma[i], 10000)\n",
    "                for j in range(99):\n",
    "                    pred[i][j] = np.percentile(tmp, j+1)\n",
    "\n",
    "            test = np.sum(series, axis=1)[-168:]\n",
    "\n",
    "            error_qs[times-1, month-1, n_clusters-1] = qloss(np.tile(test, (99,1)).T, pred)\n",
    "            error_ws_90[times-1, month-1, n_clusters-1] = ws(0.1, pred[:, 4], pred[:, 94], test)\n",
    "            error_ws_50[times-1, month-1, n_clusters-1] = ws(0.5, pred[:, 24], pred[:, 74], test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.median(error_ws_90, axis=0)[7, :]).to_csv('tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in trange(1, 13):\n",
    "    for n_clusters in range(1, 2):\n",
    "\n",
    "        path_cluster = os.path.join(path, 'result', data_set, 'clustering', 'point', 'kmeans', f'n_clusters_{n_clusters}.csv')\n",
    "        clusters = pd.read_csv(path_cluster, header=None)\n",
    "\n",
    "        series = data[:, month-1, :months[month-1]*24].T.copy()\n",
    "\n",
    "        window_size = 192\n",
    "        stride_size = 24\n",
    "\n",
    "        total_time = series.shape[0]\n",
    "        num_series = series.shape[1]\n",
    "\n",
    "        weather = get_weather(path, data_set, month)\n",
    "        week = get_dow(data_set, month)\n",
    "        day = get_hod(month)\n",
    "\n",
    "        num_covariates = 4\n",
    "        covariates = np.zeros((num_covariates, len(series)))\n",
    "        covariates[1] = stats.zscore(weather)\n",
    "        covariates[2] = stats.zscore(week)\n",
    "        covariates[3] = stats.zscore(day)\n",
    "        cov_age = stats.zscore(np.arange(total_time))\n",
    "        covariates[0] = cov_age\n",
    "        covariates = covariates.T.copy()\n",
    "\n",
    "        # params\n",
    "        json_path = os.path.join(path, 'forecasting', 'deepar', 'params24.json')\n",
    "        params = utils.Params(json_path)\n",
    "\n",
    "        params.num_class = n_clusters\n",
    "        params.relative_metrics = False\n",
    "        params.sampling = False\n",
    "        params.one_step = True\n",
    "\n",
    "        model_dir = os.path.join(path, 'result', data_set, 'forecasting', 'deepar', f'times_{1}', method)\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        params.model_dir = os.path.join(model_dir, f'n_clusters_{n_clusters}_month_{month}.pth.tar')\n",
    "\n",
    "        # use GPU if available\n",
    "        cuda_exist = torch.cuda.is_available()\n",
    "\n",
    "        # Set random seeds for reproducible experiments if necessary\n",
    "        if cuda_exist:\n",
    "            params.device = torch.device('cuda')\n",
    "            # torch.cuda.manual_seed(240)\n",
    "            model = net.Net(params).cuda()\n",
    "        else:\n",
    "            params.device = torch.device('cpu')\n",
    "            # torch.manual_seed(230)\n",
    "            model = net.Net(params)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "        loss_fn = net.loss_fn\n",
    "\n",
    "        restore_file = None\n",
    "\n",
    "        model = net.Net(params)\n",
    "        utils.load_checkpoint(params.model_dir, model)\n",
    "\n",
    "        total_mu = []\n",
    "        total_sigma = []\n",
    "\n",
    "        for day in range(1, 8):\n",
    "            if day != 7:\n",
    "                test_data = series[-168-(8-day)*24:-(8-day)*24+24, :].copy()\n",
    "                cov = covariates[-168-(8-day)*24:-(8-day)*24+24, :].copy()\n",
    "            else:\n",
    "                test_data = series[-192:, :].copy()\n",
    "                cov = covariates[-192:, :].copy()\n",
    "\n",
    "            test_x_input, test_v_input, test_label = prep_data(test_data, cov, window_size, stride_size, num_covariates, num_series, clusters[month-1], train=False)\n",
    "            test_set = TestDataset(test_x_input, test_v_input, test_label)\n",
    "            test_loader = DataLoader(test_set, batch_size=params.predict_batch, sampler=RandomSampler(test_set), num_workers=4)\n",
    "\n",
    "            test_metrics, mu, sigma = evaluate(model, loss_fn, test_loader, params, sample=params.sampling)\n",
    "\n",
    "            day_mu = torch.zeros(24)\n",
    "            day_sigma = torch.zeros(24)\n",
    "            for i in range(len(mu)):\n",
    "                day_mu = day_mu + torch.sum(mu[i], axis=0)\n",
    "                day_sigma = day_sigma + torch.sum(sigma[i]**2, axis=0)\n",
    "            total_mu.append(day_mu.numpy())\n",
    "            total_sigma.append(torch.sqrt(day_sigma).numpy())\n",
    "\n",
    "        total_mu = np.array(total_mu).reshape(-1)\n",
    "        total_sigma = np.array(total_sigma).reshape(-1)\n",
    "\n",
    "        pred = np.zeros((168, 99))\n",
    "\n",
    "        for i in range(len(total_mu)):\n",
    "            tmp = np.random.normal(total_mu[i], total_sigma[i], 10000)\n",
    "            for j in range(99):\n",
    "                pred[i][j] = np.percentile(tmp, j+1)\n",
    "\n",
    "        test = np.sum(series, axis=1)[-168:]\n",
    "\n",
    "        error_qs[0, month-1, 0] = qloss(np.tile(test, (99,1)).T, pred)\n",
    "        error_ws_90[0, month-1, 0] = ws(0.1, pred[:, 4], pred[:, 94], test)\n",
    "        error_ws_50[0, month-1, 0] = ws(0.5, pred[:, 24], pred[:, 74], test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_qs[0, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_ws_50[0, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_ws_90[0, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
