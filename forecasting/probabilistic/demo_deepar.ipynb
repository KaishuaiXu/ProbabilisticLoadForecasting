{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import trange\n",
    "\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from math import sqrt\n",
    "from pandas import read_csv, DataFrame\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prep_data(data, covariates, data_start, train = True):\n",
    "\n",
    "    time_len = data.shape[0]  # 32136\n",
    "\n",
    "    input_size = window_size-stride_size  # (192 - 24)\n",
    "    \n",
    "    windows_per_series = np.full((num_series), (time_len-input_size) // stride_size)\n",
    "    if train: windows_per_series -= (data_start+stride_size-1) // stride_size\n",
    "\n",
    "    total_windows = np.sum(windows_per_series)\n",
    "    x_input = np.zeros((total_windows, window_size, 1 + num_covariates + 1), dtype='float32')\n",
    "    label = np.zeros((total_windows, window_size), dtype='float32')\n",
    "    v_input = np.zeros((total_windows, 2), dtype='float32')\n",
    "    #cov = 3: ground truth + age + day_of_week + hour_of_day + num_series\n",
    "    #cov = 4: ground truth + age + day_of_week + hour_of_day + month_of_year + num_series\n",
    "    count = 0\n",
    "    if not train:  # for test\n",
    "        covariates = covariates[-time_len:]\n",
    "    for series in trange(num_series):  #穷举series\n",
    "        \n",
    "        cov_age = stats.zscore(np.arange(total_time-data_start[series]))  # 序列中的第几个，位置变量\n",
    "        if train:\n",
    "            covariates[data_start[series]:time_len, 0] = cov_age[:time_len-data_start[series]]  # 序列中的第几个，位置变量（矫正当前序列）\n",
    "        else:\n",
    "            covariates[:, 0] = cov_age[-time_len:]\n",
    "        \n",
    "        for i in range(windows_per_series[series]):  # 穷举window\n",
    "            if train:\n",
    "                window_start = stride_size*i+data_start[series]\n",
    "            else:\n",
    "                window_start = stride_size*i\n",
    "            window_end = window_start + window_size\n",
    "\n",
    "            x_input[count, 1:, 0] = data[window_start:window_end-1, series]\n",
    "            x_input[count, :, 1:1+num_covariates] = covariates[window_start:window_end, :]\n",
    "            x_input[count, :, -1] = series  # 序列标签\n",
    "            label[count, :] = data[window_start:window_end, series]\n",
    "            nonzero_sum = (x_input[count, 1:input_size, 0]!=0).sum()  # 个数\n",
    "            if nonzero_sum == 0:\n",
    "                v_input[count, 0] = 0\n",
    "            else:\n",
    "                v_input[count, 0] = np.true_divide(x_input[count, 1:input_size, 0].sum(),nonzero_sum)+1\n",
    "                x_input[count, :, 0] = x_input[count, :, 0]/v_input[count, 0]\n",
    "                if train:\n",
    "                    label[count, :] = label[count, :]/v_input[count, 0]\n",
    "            count += 1\n",
    "    prefix = os.path.join(save_path, 'train_' if train else 'test_')\n",
    "    np.save(prefix+'data_'+save_name, x_input)\n",
    "    np.save(prefix+'v_'+save_name, v_input)\n",
    "    np.save(prefix+'label_'+save_name, label)\n",
    "\n",
    "def gen_covariates(times, num_covariates):\n",
    "    covariates = np.zeros((times.shape[0], num_covariates))\n",
    "    for i, input_time in enumerate(times):\n",
    "        covariates[i, 1] = input_time.weekday()\n",
    "        covariates[i, 2] = input_time.hour\n",
    "        covariates[i, 3] = input_time.month\n",
    "    for i in range(1,num_covariates):\n",
    "        covariates[:,i] = stats.zscore(covariates[:,i])\n",
    "    return covariates[:, :num_covariates]\n",
    "\n",
    "def visualize(data, week_start):  \n",
    "    x = np.arange(window_size)\n",
    "    f = plt.figure()\n",
    "    plt.plot(x, data[week_start:week_start+window_size], color='b')\n",
    "    f.savefig(\"visual.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global save_path\n",
    "name = 'LD2011_2014.txt'\n",
    "save_name = 'elect'\n",
    "window_size = 192  # encoder + decoder?\n",
    "stride_size = 24\n",
    "num_covariates = 4\n",
    "train_start = '2011-01-01 00:00:00'\n",
    "train_end = '2014-08-31 23:00:00'\n",
    "test_start = '2014-08-25 00:00:00' # need additional 7 days as given input e.g., 1 - 20训练，14 - 27测试\n",
    "test_end = '2014-09-07 23:00:00'\n",
    "pred_days = 7\n",
    "given_days = 7\n",
    "\n",
    "save_path = os.path.join('data', save_name)\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "csv_path = os.path.join(save_path, name)\n",
    "if not os.path.exists(csv_path):\n",
    "    zipurl = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00321/LD2011_2014.txt.zip'\n",
    "    with urlopen(zipurl) as zipresp:\n",
    "        with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "            zfile.extractall(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(csv_path, sep=\";\", index_col=0, parse_dates=True, decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame.resample('1H',label = 'left',closed = 'right').sum()[train_start:test_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.fillna(0, inplace=True)\n",
    "covariates = gen_covariates(data_frame[train_start:test_end].index, num_covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_frame[train_start:train_end].values\n",
    "test_data = data_frame[test_start:test_end].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_start = (train_data!=0).argmax(axis=0)  #find first nonzero value in each time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = data_frame.shape[0] #32304\n",
    "num_series = data_frame.shape[1] #370\n",
    "prep_data(train_data, covariates, data_start)\n",
    "prep_data(test_data, covariates, data_start, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_time = data_frame.shape[0] #32304\n",
    "# num_series = data_frame.shape[1] #370\n",
    "\n",
    "# data = test_data\n",
    "# train= False\n",
    "# time_len = data.shape[0]  # 32136\n",
    "\n",
    "# input_size = window_size-stride_size  # (192 - 24)\n",
    "\n",
    "# windows_per_series = np.full((num_series), (time_len-input_size) // stride_size)\n",
    "# if train: windows_per_series -= (data_start+stride_size-1) // stride_size\n",
    "\n",
    "# total_windows = np.sum(windows_per_series)\n",
    "# x_input = np.zeros((total_windows, window_size, 1 + num_covariates + 1), dtype='float32')\n",
    "# label = np.zeros((total_windows, window_size), dtype='float32')\n",
    "# v_input = np.zeros((total_windows, 2), dtype='float32')\n",
    "# #cov = 3: ground truth + age + day_of_week + hour_of_day + num_series\n",
    "# #cov = 4: ground truth + age + day_of_week + hour_of_day + month_of_year + num_series\n",
    "# count = 0\n",
    "# if not train:  # for test\n",
    "#     covariates = covariates[-time_len:]\n",
    "# for series in trange(num_series):  #穷举series\n",
    "\n",
    "#     cov_age = stats.zscore(np.arange(total_time-data_start[series]))  # 序列中的第几个，位置变量\n",
    "#     if train:\n",
    "#         covariates[data_start[series]:time_len, 0] = cov_age[:time_len-data_start[series]]  # 序列中的第几个，位置变量（矫正当前序列）\n",
    "#     else:\n",
    "#         covariates[:, 0] = cov_age[-time_len:]\n",
    "\n",
    "#     for i in range(windows_per_series[series]):  # 穷举window\n",
    "#         if train:\n",
    "#             window_start = stride_size*i+data_start[series]\n",
    "#         else:\n",
    "#             window_start = stride_size*i\n",
    "#         window_end = window_start + window_size\n",
    "\n",
    "#         x_input[count, 1:, 0] = data[window_start:window_end-1, series]\n",
    "#         x_input[count, :, 1:1+num_covariates] = covariates[window_start:window_end, :]\n",
    "#         x_input[count, :, -1] = series  # 序列标签\n",
    "#         label[count, :] = data[window_start:window_end, series]\n",
    "#         nonzero_sum = (x_input[count, 1:input_size, 0]!=0).sum()  # 个数\n",
    "#         if nonzero_sum == 0:\n",
    "#             v_input[count, 0] = 0\n",
    "#         else:\n",
    "#             v_input[count, 0] = np.true_divide(x_input[count, 1:input_size, 0].sum(),nonzero_sum)+1\n",
    "#             x_input[count, :, 0] = x_input[count, :, 0]/v_input[count, 0]\n",
    "#             if train:\n",
    "#                 label[count, :] = label[count, :]/v_input[count, 0]\n",
    "#         count += 1\n",
    "# prefix = os.path.join(save_path, 'train_' if train else 'test_')\n",
    "# np.save(prefix+'data_'+save_name, x_input)\n",
    "# np.save(prefix+'v_'+save_name, v_input)\n",
    "# np.save(prefix+'label_'+save_name, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "import torch\n",
    "\n",
    "logger = logging.getLogger('DeepAR.Data')\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data_path, data_name, num_class):\n",
    "        self.data = np.load(os.path.join(data_path, f'train_data_{data_name}.npy'))  # f-string字符串，可直接代入数值\n",
    "        self.label = np.load(os.path.join(data_path, f'train_label_{data_name}.npy'))\n",
    "        self.train_len = self.data.shape[0]\n",
    "        logger.info(f'train_len: {self.train_len}')\n",
    "        logger.info(f'building datasets from {data_path}...')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.train_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index,:,:-1],int(self.data[index,0,-1]), self.label[index])\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_path, data_name, num_class):\n",
    "        self.data = np.load(os.path.join(data_path, f'test_data_{data_name}.npy'))\n",
    "        self.v = np.load(os.path.join(data_path, f'test_v_{data_name}.npy'))\n",
    "        self.label = np.load(os.path.join(data_path, f'test_label_{data_name}.npy'))\n",
    "        self.test_len = self.data.shape[0]\n",
    "        logger.info(f'test_len: {self.test_len}')\n",
    "        logger.info(f'building datasets from {data_path}...')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.test_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index,:,:-1],int(self.data[index,0,-1]),self.v[index],self.label[index])\n",
    "\n",
    "class WeightedSampler(Sampler):\n",
    "    def __init__(self, data_path, data_name, replacement=True):\n",
    "        v = np.load(os.path.join(data_path, f'train_v_{data_name}.npy'))\n",
    "        self.weights = torch.as_tensor(np.abs(v[:,0]) / np.sum(np.abs(v[:,0])), dtype=torch.double)\n",
    "        logger.info(f'weights: {self.weights}')\n",
    "        self.num_samples = self.weights.shape[0]\n",
    "        logger.info(f'num samples: {self.num_samples}')\n",
    "        self.replacement = replacement\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(torch.multinomial(self.weights, self.num_samples, self.replacement).tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import RandomSampler\n",
    "sampler = WeightedSampler('/Users/kaishuai/Desktop/ProbabilisticLoadForecasting/forecasting/probabilistic/data/elect', 'elect')\n",
    "test_set = TestDataset('/Users/kaishuai/Desktop/ProbabilisticLoadForecasting/forecasting/probabilistic/data/elect', 'elect', 370)\n",
    "test_loader = DataLoader(test_set, batch_size=32, sampler=RandomSampler(test_set), num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = WeightedSampler('/Users/kaishuai/Desktop/ProbabilisticLoadForecasting/forecasting/probabilistic/data/elect', 'elect')\n",
    "train_set = TrainDataset('/Users/kaishuai/Desktop/ProbabilisticLoadForecasting/forecasting/probabilistic/data/elect', 'elect', 370)\n",
    "train_loader = DataLoader(train_set, batch_size=32, sampler=sampler, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (test_batch, id_batch, v, labels) in enumerate(test_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([192, 32, 5])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(params.test_predict_start):  # encoder\n",
    "    # if z_t is missing, replace it by output mu from the last time step\n",
    "    zero_index = (test_batch[t,:,0] == 0)\n",
    "    if t > 0 and torch.sum(zero_index) > 0:\n",
    "        test_batch[t,zero_index,0] = mu[zero_index]\n",
    "\n",
    "    mu, sigma, hidden, cell = model(test_batch[t].unsqueeze(0), id_batch, hidden, cell)\n",
    "    input_mu[:,t] = v_batch[:, 0] * mu + v_batch[:, 1]  # v_batch[:, 1] == 0, useless\n",
    "    input_sigma[:,t] = v_batch[:, 0] * sigma\n",
    "\n",
    "test_batch[params.test_predict_start, :, 0] = input_mu[:, params.test_predict_start-1]\n",
    "\n",
    "# decoder\n",
    "if sample:\n",
    "    samples, sample_mu, sample_sigma = model.test(test_batch, v_batch, id_batch, hidden, cell, sampling=True)\n",
    "    raw_metrics = utils.update_metrics(raw_metrics, input_mu, input_sigma, sample_mu, labels, params.test_predict_start, samples, relative = params.relative_metrics)\n",
    "else:\n",
    "    sample_mu, sample_sigma = model.test(test_batch, v_batch, id_batch, hidden, cell)\n",
    "    raw_metrics = utils.update_metrics(raw_metrics, input_mu, input_sigma, sample_mu, labels, params.test_predict_start, relative = params.relative_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = nn.Embedding(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1172,  1.0811, -0.5033, -1.0248,  0.4967],\n",
       "        [-0.2623,  0.0701,  0.8654, -0.7001,  1.1395]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "embeds.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.Tensor([[1.0,2,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7811ed0969c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "embeds(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
