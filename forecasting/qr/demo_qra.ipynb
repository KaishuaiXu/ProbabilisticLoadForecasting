{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.layers import Dense, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from tqdm import trange\n",
    "import tensorflow as tf\n",
    "\n",
    "from dataloader import get_data, get_weather, get_hod, get_dow, get_train_set_qra, get_test_set_qra\n",
    "\n",
    "months = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qloss(y_true, y_pred, q):\n",
    "    tmp1 = (q / 100 - 1) * (y_true - y_pred)\n",
    "    tmp2 = q / 100 * (y_true - y_pred)\n",
    "    return K.mean(K.maximum(tmp1, tmp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 918/918 [01:03<00:00, 14.36it/s]\n"
     ]
    }
   ],
   "source": [
    "data_set = 'Irish_2010'\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "\n",
    "data = get_data(path, data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qloss(y_true, y_pred, q):\n",
    "    tmp1 = (q / 100 - 1) * (y_true - y_pred)\n",
    "    tmp2 = q / 100 * (y_true - y_pred)\n",
    "    return K.mean(K.maximum(tmp1, tmp2))\n",
    "\n",
    "def train_model_1(train, test, week, day, num_best=8):\n",
    "    \n",
    "    # to get the num of samples\n",
    "    max_lag = 24\n",
    "    max_d = 2\n",
    "    trainX, trainTlag, trainTd, trainY = get_train_set_qra(train, week, day, max_lag, max_d)\n",
    "    n_samples = trainY.shape[0]\n",
    "    \n",
    "    error_train_step1 = np.zeros((10, 24, 2))\n",
    "    error_test_step1 = np.zeros((10, 24, 2))\n",
    "    pred_train = np.zeros((24, 2, n_samples))\n",
    "    pred_test = np.zeros((24, 2, 168))\n",
    "    \n",
    "    for pat in trange(10):\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=(pat+1)*10)\n",
    "\n",
    "        for lag in trange(1, 25):\n",
    "            for d in range(1, 3):\n",
    "\n",
    "                trainX, trainTlag, trainTd, trainY = get_train_set_qra(train, week, day, lag, d)\n",
    "                testX, testTlag, testTd, testY = get_test_set_qra(train, test, week, day, lag, d)\n",
    "\n",
    "                ## QRA step 1\n",
    "                # linear model\n",
    "                inputs = Input((7 + 24 + 3 + lag*3 + d*3,), name='input')\n",
    "                x = Dense(1, use_bias=True, kernel_initializer='he_normal', bias_initializer='he_normal')(inputs)\n",
    "                model = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "                # Train\n",
    "                model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['accuracy'])\n",
    "                hist1 = model.fit(x=np.hstack((trainX, trainTlag, trainTd)), y=trainY, validation_split=0.2, epochs=1500, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "                # Predict (train)\n",
    "                pred = model.predict(x=np.hstack((trainX, trainTlag, trainTd)))\n",
    "                error_train_step1[pat, lag-1, d-1] = np.sum(np.abs(pred[-n_samples:, :] - trainY[-n_samples:, :]))\n",
    "                pred_train[lag-1, d-1] = np.squeeze(pred[-n_samples:, :])\n",
    "\n",
    "                # Predict (test)\n",
    "                pred = model.predict(x=np.hstack((testX, testTlag, testTd)))\n",
    "                error_test_step1[pat, lag-1, d-1] = np.sum(np.abs(pred - testY))\n",
    "                pred_test[lag-1, d-1] = np.squeeze(pred)\n",
    "    \n",
    "    # prepare for step 2\n",
    "#     series_train_1 = pred_train[np.argsort(error_train_step1[:,0])[:num_best//2], 0]\n",
    "#     series_train_2 = pred_train[np.argsort(error_train_step1[:,1])[:num_best//2], 1]\n",
    "\n",
    "#     trainX_ = np.vstack((series_train_1, series_train_2)).T\n",
    "#     trainY_ = trainY[-n_samples:, :].copy()\n",
    "    \n",
    "#     series_test_1 = pred_test[np.argsort(error_train_step1[:,0])[:num_best//2], 0]\n",
    "#     series_test_2 = pred_test[np.argsort(error_train_step1[:,1])[:num_best//2], 1]\n",
    "    \n",
    "#     testX_ = np.vstack((series_test_1, series_test_2)).T\n",
    "#     testY_ = testY\n",
    "    \n",
    "    # clear\n",
    "    del model, pred, hist1\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "#     return trainX_, trainY_, testX_, testY_\n",
    "    return error_train_step1, error_test_step1\n",
    "\n",
    "def train_model_2(trainX_, trainY_, testX_, num_best):\n",
    "    \n",
    "    total_pred = []\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "    \n",
    "    for q in trange(1, 100):\n",
    "        \n",
    "        input_dim = num_best\n",
    "        model = Sequential([Dense(1, use_bias=True, kernel_initializer='he_normal', bias_initializer='he_normal', input_shape=(input_dim,))])\n",
    "\n",
    "        # Train\n",
    "        model.compile(loss=lambda y_true, y_pred: qloss(y_true, y_pred, q), optimizer='adam')\n",
    "        hist2 = model.fit(x=trainX_, y=trainY_, validation_split=0.2, epochs=1000, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "        # Predict (test)\n",
    "        pred = model.predict(x=testX_)\n",
    "        total_pred.append(np.squeeze(pred))\n",
    "    \n",
    "    total_pred = np.array(total_pred)\n",
    "    \n",
    "    del model, pred, hist2\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    return total_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = 1\n",
    "n_clusters = 2\n",
    "method = 'hierarchical/euclidean'\n",
    "\n",
    "path_cluster = os.path.join(path, 'result', data_set, 'clustering', 'point', method, f'n_clusters_{n_clusters}.csv')\n",
    "clusters = pd.read_csv(path_cluster, header=None)\n",
    "\n",
    "series = data[:, month-1, :months[month-1]*24]\n",
    "weather = get_weather(path, data_set, month)\n",
    "week = get_dow(data_set, month)\n",
    "day = get_hod(month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 1/24 [00:03<01:23,  3.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 2/24 [00:09<01:35,  4.32s/it]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▎        | 3/24 [00:13<01:25,  4.09s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 4/24 [00:17<01:23,  4.15s/it]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 5/24 [00:20<01:14,  3.93s/it]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 6/24 [00:25<01:15,  4.22s/it]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 7/24 [00:30<01:12,  4.29s/it]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 8/24 [00:37<01:21,  5.08s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 9/24 [00:44<01:25,  5.72s/it]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 10/24 [00:48<01:13,  5.23s/it]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 11/24 [00:52<01:02,  4.82s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 12/24 [00:55<00:50,  4.23s/it]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 13/24 [00:59<00:46,  4.24s/it]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 14/24 [01:03<00:40,  4.09s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▎   | 15/24 [01:07<00:36,  4.06s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 16/24 [01:11<00:32,  4.07s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 17/24 [01:16<00:30,  4.33s/it]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 18/24 [01:20<00:25,  4.19s/it]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 19/24 [01:24<00:21,  4.34s/it]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 20/24 [01:28<00:16,  4.22s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 21/24 [01:33<00:13,  4.40s/it]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 22/24 [01:38<00:09,  4.71s/it]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 23/24 [01:46<00:05,  5.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 24/24 [01:50<00:00,  5.24s/it]\u001b[A\u001b[A\n",
      " 10%|█         | 1/10 [01:50<16:37, 110.81s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 1/24 [00:07<02:57,  7.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 2/24 [00:15<02:52,  7.84s/it]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▎        | 3/24 [00:20<02:27,  7.00s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 4/24 [00:31<02:39,  8.00s/it]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 5/24 [00:36<02:13,  7.05s/it]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 6/24 [00:45<02:17,  7.63s/it]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 7/24 [00:55<02:22,  8.41s/it]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 8/24 [01:00<01:58,  7.39s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 9/24 [01:05<01:42,  6.86s/it]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 10/24 [01:11<01:30,  6.50s/it]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 11/24 [01:23<01:45,  8.15s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 12/24 [01:31<01:35,  7.97s/it]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 13/24 [01:38<01:25,  7.77s/it]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 14/24 [01:43<01:09,  6.95s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▎   | 15/24 [01:48<00:56,  6.31s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 16/24 [01:54<00:50,  6.25s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 17/24 [01:58<00:39,  5.67s/it]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 18/24 [02:03<00:32,  5.43s/it]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 19/24 [02:08<00:26,  5.37s/it]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 20/24 [02:13<00:20,  5.02s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 21/24 [02:20<00:17,  5.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 22/24 [02:27<00:11,  5.98s/it]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 23/24 [02:32<00:05,  5.70s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 24/24 [02:38<00:00,  5.96s/it]\u001b[A\u001b[A\n",
      " 20%|██        | 2/10 [04:29<16:41, 125.19s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 1/24 [00:05<02:10,  5.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 2/24 [00:16<02:36,  7.13s/it]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▎        | 3/24 [00:21<02:19,  6.63s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 4/24 [00:29<02:21,  7.08s/it]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 5/24 [00:35<02:07,  6.69s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "for i in range(n_clusters):\n",
    "\n",
    "    index = list(clusters[month-1] == i)\n",
    "    sub_series = series[index]\n",
    "    sub_series = np.sum(sub_series, axis=0)\n",
    "\n",
    "    total_series = np.vstack((sub_series, weather))\n",
    "\n",
    "    test = total_series[:, -168:]\n",
    "    train = total_series[:, :-168]\n",
    "\n",
    "    scale = np.zeros(2)\n",
    "    scale[0] = np.max(train[0])\n",
    "    scale[1] = np.min(train[0])\n",
    "    train[0] = (train[0] - scale[1]) / (scale[0] - scale[1])\n",
    "    test[0] = (test[0] - scale[1]) / (scale[0] - scale[1])\n",
    "\n",
    "    num_best = 8\n",
    "    \n",
    "#     pred_trainX_, pred_trainY_, pred_testX_, pred_testY_ = train_model_1(train, test, week, day, num_best)\n",
    "    error_train, error_test = train_model_1(train, test, week, day, num_best)\n",
    "#     pred_series = train_model_2(pred_trainX_, pred_trainY_, pred_testX_, num_best)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
